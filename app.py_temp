import os, anyio                       # anyio gives a cross-backend run()
from langchain_mcp_adapters.tools import load_mcp_tools
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from mcp.client.stdio import stdio_client
from mcp import ClientSession, StdioServerParameters
from ollama_manager import OllamaManager

class InferlessPythonModel:
    def initialize(self):
        manager = OllamaManager()
        manager.start_server()
        models = manager.list_models()
        print(f"Available models: {models}")
        
        if not any(model['name'] == 'llama3.2' for model in models):
            manager.download_model('llama3.2')

        self.llm = ChatOpenAI(base_url="http://localhost:11434/v1",
                 api_key="ollama",
                 model="llama3.2")
        self.maps_server = StdioServerParameters(
                                            command="npx",
                                            args=["-y", "@modelcontextprotocol/server-google-maps"],
                                            env={"GOOGLE_MAPS_API_KEY": "AIzaSyDrTJNscmxw8dmQFujczCKH0XBfyCRAvBE"}
                                        )

    def infer(self,inputs):
        answer = self.run_search("Can you find me some good pizza stores in HSR Layout Bangalore?")
        return {"result":"Here's the result"}

    def run_search(self,question: str):
        async def _inner():
            async with stdio_client(self.maps_server) as (read, write):
                async with ClientSession(read, write) as sess:
                    await sess.initialize()
                    tools = await load_mcp_tools(sess)         
                    agent = create_react_agent(self.llm, tools)     
                    return await agent.ainvoke({"messages": question})
        return anyio.run(_inner)                
